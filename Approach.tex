\begin{figure*}[t]
	\centering
	\includegraphics[width=2.0\columnwidth]{sti} % Reduce the figure size so that it is slightly narrower than the column. Don't use precise values for figure width.This setup will avoid overfull boxes. 
	\caption{The detailed architecture of SOVI. ENet adopts an encoder-decoder architectures with heterogeneous receptive field.
	FNet is based on the backbone Resnet-101, which is associated with ENet. TexNet utilizes a coarse-to-fine manner to inpaint the final frames. To save space, all the input masks are omitted.}
	
	%	. . FNet uses the Resnet101 as backbone, followed by a decoder. STI . In FNet, $T$ is the number of input frames, and channel $2$ denotes the motion along $x$ and $y$ axis. }
	\label{fig:stiNet}
\end{figure*}

\section{Approach}
\label{sec:approach}

The target of our method is to recover the missing contents in a corrupted video with fine details and temporal consistence.
%
%Our intuition is that there exists complementary information in neighboring frames, which can benefit the inpainting process of each individual frame.Therefore, 
In each inference batch to fill up the missing part in frame $X_t$, total $T$ frames $\boldsymbol{X}$ ($T=5$), indexed by $\msset{X}$, are fed to our inpainting network SOVI, as well as the corresponding masks $\boldsymbol{M}$ that indicate the missing regions.
The output of SOVI is the completion result \(Y_t\) at time $t$. 
%Our method infers each target frame $Y_t$ individually but with hints from its neighboring source frames.
%
To fill up the missing region with structural details and temporal coherence, our SOVI consists of two flow paths. The first is an edge-guided inpainting path, which consists of an edge inpainting network ENet that recovers missing edges of the input frames, a coarse-to-fine texture inpainting network that replenishes the appearance details under the structural guidance.
The second flow path leverages motion flows to enhance the temporal coherence of the completed frames.
It consists of a flow inpainting network FNet that predicts the motion flows in the missing regions and an ensemble module that aggregates previously synthesized frames to refine the current frame.%looks spatially and temporally consistent. 
	%While this forward pass takes multiple frames as input and only completes a single frame, 
%in the inpainted edge maps and videos in a joint learning manner. }
%As shown in Fig.~\ref{fig:stiNet}, our SOVI mainly consists of three parts: (a) an edge inpainting network that recovers structural details of the input frames, (b) a flow inpainting network that completes the dense motions in the missing regions between neighbor frames and the target frame, and (c) a spatio-temporal inpainting network that generates plausible contents that looks spatially and temporally consistent.
%The detailed implementation of each part is explained in the following sections.
%to% enforce the temporal consistency
 



\subsection{Edge Inpainting Network}
\label{sec:edgenet}

%Video inpainting suffers from the lack of structural details.
To fill the missing regions with fine details, their corresponding structural edges are predicted, and provide structural guidance for the following texture synthesis.
% reasonable clues
%The edge completion module aims to generate the completed edge maps $\boldsymbol{E}$ for input frames $\boldsymbol{X}$. 
%
Given the input frames $\boldsymbol{X}$, a canny edge detector is first used to extract edge maps $\boldsymbol{E}^{i}$. Notably, the masked regions in $\boldsymbol{E}^{i}$ are missed. 
%Given the incompleted grayscale images $\boldsymbol{X}^{g}$ of input frame, a canny edge detector is first used to generate initial edge maps . 
Then, the edge inpainting network (ENet) completes the missing edges.
The input of ENet consists of the grayscale frames $\boldsymbol{X}^{g}$, initial edge maps $\boldsymbol{E}^{i}$, and their corresponding masks $\boldsymbol{M}$.
%
As shown in Fig.~\ref{fig:stiNet}, ENet consists of a generator $G^E$ and a discriminator $D^E$.
$G^E$ is composed of a two-layer 3D encoder, eight 2D residual blocks, and a two-layer 3D decoder. 
The 3D encoder and decoder are designed to learn the spatio-temporal correlation, while the 2D residual blocks are used to enrich spatial features in larger receptive fields. The discriminator $D^E$ follows the $70\times 70$ PatchGAN architecture \cite{Isola_2017_CVPR}. 
Finally, the inpainted edge maps are obtained by:
\begin{equation}
\label{eq:edgenet}
\boldsymbol{E}=G^E(\boldsymbol{E}^{i},\boldsymbol{X}^{g},\boldsymbol{M}).
\end{equation}

The ENet is trained by playing a minimax game to optimize the generator $G^E$ and the discriminator $D^E$ as
\begin{equation}
\label{eq:loss_e}
\mathcal{O}_{edge} =\min\limits_{G^E} \max \limits_{D^E} \big(\mathcal{L}^E_{adv}+\lambda_1 * \mathcal{L}^E_{fm}+\mathcal{L}_{fec}\big),
\end{equation}
where $\mathcal{L}^E_{adv}$, $\mathcal{L}^E_{fm}$, and $\mathcal{L}_{fec}$ are respectively the adversarial loss, feature matching loss, and flow-edge consistency loss. $\mathcal{L}_{fec}$ targets to temporally smooth edge maps, which will be introduced in the following section.
$\lambda_1$ is a hyper-parameter to balance the three terms.
%
Following the adversarial learning manner, $\mathcal{L}^E_{adv}$ can facilitate ENet to produce plausible edge maps and is defined as
\begin{equation} \label{eq:edge_adver}
\mathcal{L}^E_{adv}  =\mathbb{E}\big[logD^E(\boldsymbol{E}^{gt},\boldsymbol{X}^{g})\big] +\mathbb{E} \big[log\big(1-D^E ( \boldsymbol{E},\boldsymbol{X}^{g})\big)\big],
\end{equation}
where $\boldsymbol{E}^{gt}$ represents the ground truth edge maps. $\mathcal{L}^E_{fm}$ evaluates the feature-level similarity between ground truth edge maps and predicted edge maps, which helps to create structurally rational edge maps. 
Feature matching loss was first proposed in \cite{wang2018high} and has been widely used in recent GANs.
The feature matching loss is defined by:
\begin{equation}
\label{eq:edge_fm}
\mathcal{L}^E_{fm}=\sum_{k=1}^L{\frac{1}{N_k}\left\| D^E_k(\boldsymbol{E}^{gt},\boldsymbol{X}^{g})- D^E_k(\boldsymbol{E},\boldsymbol{X}^{g})\right\|_1},
\end{equation}
where $D^E_k$ is the output of the $k$-th layer in $D^E$, while $N_k$ is the element number of $D^E_k$. 
Note that the discriminator $D^E$ is not optimized by the feature matching loss term. It plays as a feature extractor to optimize the generator $G^E$ for producing plausible edge maps $\boldsymbol{E}$.


\begin{figure}[t]
	\centering
	\includegraphics[width=1.0\columnwidth]{coars-fine} % Reduce the figure size so that it is slightly narrower than the column. Don't use precise values for figure width.This setup will avoid overfull boxes. 
	\caption{The visualization of coarse and fine inpainted frames.}
	
	%	. . FNet uses the Resnet101 as backbone, followed by a decoder. STI . In FNet, $T$ is the number of input frames, and channel $2$ denotes the motion along $x$ and $y$ axis. }
	\label{fig:coarse-fine}
\end{figure}



\subsection{Edge-Guided Texture Inpainting Network}

%Combining the inpainted edge maps $\boldsymbol{E}$ and flow maps $\boldsymbol{O}$, a spatio-temporal inpainting network (STINet) is designed to obtain the final target frame $Y_t$.

Given the completed edge maps $\boldsymbol{E}$ for the 
$T$ frames, then we fill the colors and textures of the missing regions under the structural guidance in a coarse-to-fine manner. 
%
The proposed TexNet consists of a coarse network and a refinement network, as Fig.~\ref{fig:stiNet} shows.
%
The input of TexNet is the concatenation of $\boldsymbol{X}$, $\boldsymbol{E}$, and $\boldsymbol{M}$.
First, the coarse inpainting network consists of a set of 3D convolutions to capture the temporal information, which targets to produce an rough completion for the $T$ frames  $\boldsymbol{Y}^i$ with colors.
The 3D coarse network incorporates neighboring frames by convolutions of the time dimension.
%
Second, the refinement network takes the synthesized edge maps $\boldsymbol{E}$ and $\boldsymbol{Y}^i$ as input and uses 2D convolutional layers to enhance structure details and produce a visually realistic frame $Y^I_t$with fine textures. 
An example of coarse and refine results is shown in Fig.~\ref{fig:coarse-fine}. 
For the corrupted region, our ENet first completes sparse edges which well represent the structure and shapes of the missing content, then progressively replenishes the textures under the guidance of synthesized edges.



To fully exploit the structural information in $\boldsymbol{E}$, we design a structure attention module (SAM) in the refinement network.
The core insight of the SAM is to capture the spatial correlation between structural edges and video contents.
The detailed implementation of the SAM is shown in Fig.~\ref{SEM}.
Two separate convolution blocks are first applied to embed structure features from the predicted edge maps $\boldsymbol{E}$.
% which alleviate the feature discrepancy between structural edge and video texture.
Then, the intermediate video features and embedded structural features are interacted to calculate the latent structure-texture correlation via matrix multiplication. 
After a SoftMax operation, the normalized attention map represents long-range correlation between the structure and high-level video features.
%
Finally, the normalized attention map is applied to the intermediate video features, and the structure information is thus embedded in the TexNet.
After introducing structural guidance, the inpainted content by TexNet becomes more structurally and semantically realistic.
 


To train the TexNet, we use a reconstruction loss defined as:
%
\begin{equation}
	\label{eq:1}
		\mathcal{O}_{tex}=\min\limits_{G^I} \max \limits_{D^I} \big(\mathcal{L}^{I}_{l_1}+\mathcal{L}^I_{adv}+\mathcal{L}^{I}_{flo}\big).
\end{equation}
%
The first term $\mathcal{L}^{I}_{l_1}$ is $l_1$-reconstruction loss to measure the difference between predicted video frame and the ground truth video frame $\widetilde{Y}_t$.
Different from \cite{nazeri2019edgeconnect}, we penalize both the coarse and fine predictions, given by:
\begin{equation}
	\begin{aligned}
		\mathcal{L}^{I}_{l_1}&=\frac{1}{\left\|\boldsymbol{M} \right\|_1}\left\|(\boldsymbol{Y}^i-\widetilde{\boldsymbol{Y}})\odot \boldsymbol{M}\right\|_1\\ &+\lambda_2*\frac{1}{\left\|M_t \right\|_1}\left\|(Y^I_t-\widetilde{Y}_t)\odot M_t\right\|_1.
	\end{aligned}
\end{equation}
%
%
Besides, an extra adversarial loss $\mathcal{L}^I_{adv}$ is introduced to promote the visual realism of the generated frame by:
%$\mathcal{L}^I_{adv}$ is defined as:
\begin{equation}
	\label{eq:inp_adver}
	\mathcal{L}^I_{adv}=\mathbb{E}[logD^I(\widetilde{Y}_t)]+\mathbb{E}[log\big(1-D^I(Y^I_{t})\big)],
\end{equation}
where $G^I$ is the TexNet and $D^I$ has the similar architecture with $D^E$.
% $\mathcal{L}^I_{adv}$ enforces the generated frame to be more realistic.
$\mathcal{L}^I_{flo}$ is a flow warping loss, which will be introduced subsequently.




 \begin{figure}[t]
	\centering
	\includegraphics[width=0.7\columnwidth]{SEM} % Reduce the figure size so that it is slightly narrower than the column. Don't use precise values for figure width.This setup will avoid overfull boxes. 
	\caption{Architecture of the structure attention module. $C$ is the channel of the input video features, and $N=H\times W$. $\otimes$ represents matrix multiplication. Usually, we set $C_1=C/8$.}
	\label{SEM}
\end{figure} 


\begin{table*}[t]
	\caption{Comparisons with existing methods on YouTubeVOS. To save space, the year of method is not listed.}\smallskip
	
	\centering
	\resizebox{2.0\columnwidth}{!}{
		\smallskip\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c }
			\hline
			&\multicolumn{3}{c|}{Fixed Square Mask}& \multicolumn{3}{c|}{Moving Square Mask}&\multicolumn{3}{c|}{Free-Form Mask}&Inference \\
			\cline{2-10} 
			&PSNR & SSIM & FID & PSNR & SSIM & FID & PSNR & SSIM & FID&Speed(fps))\\
			\hline
			Nazeri et al. &28.6446 &0.9484  &   38.2116  &	
			30.7478 & 0.9647 &  16.2739  &
			25.6693  & 0.9088 &  43.0366&22.81 \\
			\hline
			Wang et al. &27.9668 & 0.9515 &  40.7199  &	 
			31.5776	& 0.9678 &  13.8383&   
			32.1862 & 0.9626 & 19.1191 &8.1634 \\
			\hline
			
			
			Kim et al. b& 28.0846&0.9468 &  39.9377  & 
			36.8598	& 0.9728 &7.2315  &
			33.5549	& 0.9646 & 9.3797&1.2275  \\
			\hline
			Xu et al. &29.0531 & 0.9497 &  32.8860  & 
			37.8241& 0.9772 &6.3746  &
			32.6287 &0.9618  &  11.1501&0.5620 \\
			\hline
			
			\hline
			
			
			TexNet &28.0174 &0.9494  &  42.7164   &	
			33.8131 &  0.9705  &8.2390	& 
			30.0680& 0.9390 & 20.6358&7.6335
			\\
			\hline
			+edge input  &29.5242 &  0.9520& 36.2097   &	
			37.6630	& 0.9798 &3.5161    &	
			33.8206	&0.9659  &    6.6651& 5.2356 \\
			\hline
			
			+SAM &29.9918 &  0.9533 &  27.4198  &	
			38.2433	& 0.9807 &   2.5083  &	
			35.7783	&0.9712  &   5.8786 & 5.1546\\
			\hline
			
			
			
			
			Ours &\textbf{30.0590} &\textbf{0.9543}&   \textbf{27.2431} &
			\textbf{38.8186} & \textbf{0.9824} & \textbf{2.3455} &
			\textbf{35.9613}  & \textbf{0.9721}&  \textbf{ 5.8694} &2.5643\\
			
			\hline
			
			
		\end{tabular}
	}
	\label{tab:sem}
\end{table*}







\subsection{Flow-Guided Temporal Coherence Enhancement}
\label{sec:fec}
It is vital to maintain temporal consistency in video inpainting.
%Optical flows are widely used to represent the dense correspondence between frames.
To enhance temporal consistency in the synthesized results, we employ optical flows to smooth the inpainted edge maps and synthesized frames from ENet and TexNet.
% and reinforce the temporal coherence between neighboring edge maps and frames during training. 
Moreover, we design an ensemble module which leverages the previous inpainted frames to refine the current frame completion, as Fig.~\ref{fig:stiNet} shows. 



To this end, we design a flow inpainting network (FNet) to predict the motion tendency in the missing region.
%
Similar to ENet, a set of initial flow maps \(\boldsymbol{O}^i\) for pairs between the current frame $X_t$ and its neighboring frames are first generated using a flow extraction network, such as FlowNet2.0~\cite{Flownet_2017_CVPR}.
Notably, \(\boldsymbol{O}^i\) consists of four flow maps \((O^i_{t\Rightarrow t-7}, O^i_{t\Rightarrow t-3}, O^i_{t\Rightarrow t+3}, O^i_{t\Rightarrow t+7})\).
From the predicted optical flows $\boldsymbol{O}^i$, the proposed FNet synthesizes the flows in the missing regions to obtain \(\boldsymbol{O}\) as:
\begin{equation}
	\label{eq:flownet}
	\boldsymbol{O}=G^F(\boldsymbol{O}^{i},\boldsymbol{M}),
\end{equation}
where the generator $G^F$ consists of an encoder that uses ResNet101 \cite{He_2016_CVPR} as backbone and by a decoder.
%The detailed architecture of FNet is shown in Fig.~\ref{fig:stiNet}.

To train the flow inpainting network, the loss function is given by:
\begin{equation}
	\label{eq:flow_all}
	\mathcal{O}_{flow}=\min\limits_{G^F} \big(\mathcal{L}^F_{rec}+ \mathcal{L}^F_{har}+\mathcal{L}_{fec}\big),
\end{equation}
where $\mathcal{L}^F_{rec}$ and $\mathcal{L}^F_{har}$ are respectively $l_1$ loss and hard example mining loss in the missing regions, following the definition in \cite{Xu_2019_CVPR}. 
Specifically, $\mathcal{L}^F_{har}$ encourages the network to focus on those hard samples in order to avoid blurry texture.




To enhance temporal consistency in the synthesized results, we apply the completed flows to warp the inpainted edge maps and compute the consistency between neighboring edge maps. 
%
Instead of separately training two subnetworks ENet and FNet, we train them jointly, because the temporal correlation and structural details can boost each other. 
To achieve this goal, a flow-edge consistency loss is defined as:
%
\begin{equation}
	\label{eq:flow_edge}
	\mathcal{L}_{fec}=\sum_{k}\frac{1}{\left\|M_{t} \right\|_1}\left\|(E_{t}-\phi(O_{t\Rightarrow t-k},E_{t-k}))\odot M_{t}\right\|_1,
\end{equation}
where $\phi(O_{t\Rightarrow t-k},E_{t-k})$ is the warping operation which warps the edge map $E_{t-k}$ to the target frame according to the generated optical flow $O_{t\Rightarrow t-k}$.
$k$ denotes the index of neighboring frames ($k\in \left\{-7,-3,+3,+7 \right\}$). 
%Specifically, $\mathcal{L}_{fec}$ warps the edge maps from neighboring frames to the target frame to penalize the reconstruction loss.
%In terms of ENet, $\mathcal{L}_{fec}$ encourages the predicted edge maps to be temporally smoothing, which should conform to the motion tendency in the flow maps $\boldsymbol{O}$. 
%In terms of FNet, $\mathcal{L}_{fec}$ constrains the model to focus on edges.
%Thus, the total loss function for joint training of ENet and FNet is:
%\begin{equation}
%	\label{eq:flow}
%	\mathcal{L}_{joint}=\mathcal{L}_{edge}+\mathcal{L}_{flow}+ \mathcal{L}_{fec}.
%\end{equation}


To eliminate temporal flickers in the completed video frames, we enforce the temporal coherence of synthesized neighboring frames via a flow warping constraint $\mathcal{L}^I_{flo}$ in Eq.~\eqref{eq:1}. 
%
Specifically, the neighboring frames are warped into the target frame: 
\begin{equation}
\label{eq:inp_flow}
\mathcal{L}^I_{flo}=\sum_{\widehat{t}\in\mathcal{T}}\left\| Y^I_t-\phi(O_{t\Rightarrow \widehat{t}},Y_{\widehat{t}}) \right\|_1,
\end{equation}
where $\mathcal{T}=\{t-7,t-3\}$. $\phi(O_{t\Rightarrow \widehat{t}},Y_{\widehat{t}})$ warps $Y_{\widehat{t}}$ to $Y^I_{t}$ using flow $O_{t\Rightarrow \widehat{t}}$.
$O_{t\Rightarrow \widehat{t}}$ is predicted using the subsequent flow inpainting network.
$\mathcal{L}^I_{flo}$ expects that all the neighboring frames can be well warped to the target frame with small reconstruction loss.

Finally, a temporal ensemble module is designed to aggregate the temporal neighboring frames and generate the final current frame $Y_t$. The architecture is shown in Fig.~\ref{fig:stiNet}. The module is trained with a $l_1$ reconstruction loss and an adversarial loss.







