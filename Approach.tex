\section{Approach}
\label{sec:approach}

The target of STSENet is to recover the missing contents in a corrupted video.
In each inference batch, to generate current frame \(Y_t\), total $5$ frames $\boldsymbol{X}$, indexed by $\msset{X}$, are fed to STSENet, as well as corresponding masks $\boldsymbol{M}$, to inpaint current frame \(Y_t\).
$\boldsymbol{M}$ is the mask sequence that indicates the missing regions in $\boldsymbol{X}$, and \( \widetilde{Y}_t\) is the ground truth of current frame.
%Our method infers each target frame $Y_t$ individually but with hints from its neighboring source frames.
%
The intuition is that there exists complementary information in the neighboring frames, which can benefit the inpainting process of the current frame.
As shown in Fig.~\ref{fig:stiNet}, our STSENet mainly consists of three parts: (a) an edge inpainting network that recovers structural details of input frames, (b) a flow inpainting network that completes the dense motions in the missing regions between neighbor frames and the target frame, and (c) a spatio-temporal inpainting network that generates plausible contents that looks spatially and temporally consistent.
The detailed implementation of each part will be illustrated in the following sections.


%For convenience, \( \widetilde{Y}_{1:T}\) is denoted as the ground truth video, and \(M_{1:T}\) is the mask sequence that indicates the missing regions in \(X_{1:T}\).
%In each inference batch, to generate current frame \(Y_t\), total $5$ frames $\boldsymbol{X}$, indexed by $\msset{X}$, are fed to STSENet, as well as corresponding mask $\boldsymbol{M}$.  

%\cxj{use edge inpainting subnetwork and flow inpainting subnetwork to keep the same with Fig1?}








\subsection{Edge Inpainting Network}
\label{sec:edgenet}

Video inpainting suffers from the lack of structural details.
To inpaint the missing regions with fine details, we first predict corresponding reasonable structural clues, which are beneficial to the following frame inpainting process.
%The edge completion module aims to generate the completed edge maps $\boldsymbol{E}$ for input frames $\boldsymbol{X}$. 
%

Given the input frames $\boldsymbol{X}$, we first employ a Canny edge detector to extract the edge maps $\boldsymbol{E}^{i}$. 
%Given the incompleted grayscale images $\boldsymbol{X}^{g}$ of input frame, a canny edge detector is first used to generate initial edge maps . 
Then, an edge inpainting network (ENet) is designed to complete the missing edges in $\boldsymbol{E}^{i}$.
The input of our ENet consists of the grayscale frames $\boldsymbol{X}^{g}$, initial edge maps $\boldsymbol{E}^{i}$, and corresponding masks $\boldsymbol{M}$.
%
The detailed implementation of ENet is shown in Fig.~\ref{fig:stiNet}, which consists of a generator $G^E$ and a discriminator $D^E$.
$G^E$ contains a 2-layer 3D encoder, eight 2D residual blocks, and a 2-layer 3D decoder. 
The 3D encoder and decoder are designed to learn spatio-temporal information, while the 2D residual blocks are used to enrich the spatial features in a larger receptive field. The discriminator $D^E$ follows the $70\times 70$ PatchGAN architecture \cite{Isola_2017_CVPR}. 
%\cxj{There is no mark for the generator and discriminator in Fig.2.}
Finally, the inpainted edge maps are obtained by:
\begin{equation}
\label{eq:edgenet}
\boldsymbol{E}=G^E(\boldsymbol{E}^{i},\boldsymbol{X}^{g},\boldsymbol{M}),
\end{equation}

The ENet is trained by playing a minimax game to optimize the generator $G^E$ and discriminator $D^E$ as
\begin{equation}
\label{eq:loss_e}
\min\limits_{G^E} \max \limits_{D^E} \big(\mathcal{L}^E_{adv}+\lambda_1 * \mathcal{L}^E_{fm}\big),
\end{equation}
where $\mathcal{L}^E_{adv}$ and $\mathcal{L}^E_{fm}$ are respectively the adversarial loss and feature matching loss. 
$\lambda_1$ is a hyper-parameter to balance the two terms.
%
Following the adversarial learning manner, $\mathcal{L}^E_{adv}$ can facilitate ENet to produce plausible edge maps, which is defined by
\begin{equation} \label{eq:edge_adver}
\mathcal{L}^E_{adv}  =\mathbb{E}\big[logD^E(\boldsymbol{E}^{gt},\boldsymbol{X}^{g})\big] +\mathbb{E} \big[log\big(1-D^E ( \boldsymbol{E},\boldsymbol{X}^{g})\big)\big],
\end{equation}
where $E^{gt}$ represents the ground truth of edge maps. $\mathcal{L}^E_{fm}$ evaluates the feature-level similarity between ground truth edge maps and predicted edge maps, which helps to create structurally rational edge maps. The feature matching loss is defined by:
\begin{equation}
\label{eq:edge_fm}
\mathcal{L}^E_{fm}=\sum_{k=1}^L{\frac{1}{N_k}\left\| D^E_k(\boldsymbol{E}^{gt},\boldsymbol{X}^{g})- D^E_k(\boldsymbol{E},\boldsymbol{X}^{g})\right\|_1},
\end{equation}
where $D^E_k$ is the output of the $k$-th layer in $D^E$, while $N_k$ is the element number of $D^E_k$. 
Note that the discriminator $D^E$ is not optimized by the feature matching loss term. It plays as a feature extractor to optimize the generator $G^E$ for producing plausible edge maps $\boldsymbol{E}$.




\subsection{Flow Inpainting Network}

It is vitally important to maintain temporal consistency in video inpainting.
To this end, the flow inpainting network (FNet)  is designed to predict the motion tendency among frames.

Similar to ENet, an initial flow \(\boldsymbol{O}^i\) for each group of neighboring frames $\boldsymbol{X}$ are first generated using flow extraction network like FlowNet2.0 \cite{Flownet_2017_CVPR}.
Notably, \(\boldsymbol{O}^i\) consists of four flow maps \((O^i_{t\Rightarrow t-7},O^i_{t\Rightarrow t-3},O^i_{t\Rightarrow t+3},O^i_{t\Rightarrow t+7})\) for the input 5 frames.
Then, the proposed FNet $G^F$ is used to complete \(\boldsymbol{O}^i\) by:
\begin{equation}
\label{eq:flownet}
\boldsymbol{O}=G^F(\boldsymbol{O}^{i},\boldsymbol{M}),
\end{equation}
where $\boldsymbol{O}$ is the predicted optical flow.
%\cxj{Xuejin: I modified $N^E$ and $N^F$ to $G^E$ and $G^O$ to avoid confusion with number symbols $N$.  }
%
The detailed architecture of FNet is shown in Fig.~\ref{fig:stiNet}.

The loss function for FNet is :
\begin{equation}
\label{eq:flow_all}
\mathcal{L}_{flow}=\mathcal{L}^F_{rec}+ \mathcal{L}^F_{har},
\end{equation}
where $\mathcal{L}^F_{rec}$ and $\mathcal{L}^F_{har}$ are respectively $l_1$-reconstruction loss and hard mining loss, which follow the settings in \cite{Xu_2019_CVPR}. Specifically, $\mathcal{L}^F_{har}$ encourages the network to focus on those hard samples, which can avoid blurred texture.
%   \cxj{same as the cvpr19paper?... reference?? remove the definition if reference is provided. }. 
%
%The $l_1$-reconstruction loss is defined as:
%\begin{equation}
%\label{eq:flow_l1}
%\mathcal{L}^F_{rec}=\frac{1}{\left\|\boldsymbol{M} \right\|_1}\left\|(\boldsymbol{O}-\boldsymbol{O}^{gt})\odot \boldsymbol{M}\right\|_1,
%\end{equation}
%where $\boldsymbol{O}^{gt}$ is the ground truth. The symbol $\odot$ denotes the pixel-wise multiplication. Specifically, $\mathcal{L}^F_{rec}$ measures the difference between $\boldsymbol{O}^{gt}$ and $\boldsymbol{O}$.
%The hard mining loss is defined as:
%\begin{equation}
%\label{eq:flow_hard}
%\mathcal{L}^F_{har}=\frac{1}{\left\|\boldsymbol{M}_H \right\|_1}\left\|(\boldsymbol{O}-\boldsymbol{O}^{gt})\odot \boldsymbol{M}_H\right\|_1,
%\end{equation}
%where $\boldsymbol{M}_H$ is the binary mask of hard example regions.
%The hard mining loss encourages the network to focus on those hard samples, which can avoid blurred texture. 

\subsection{Flow-Edge Consistency Loss}
Instead of separately training the two subnetworks ENet and FNet, we train the two networks jointly, because the temporal information and structural details can boost each other.
%, thereby rendering them to be mutually improved.
%Thus we will obtain detail-aware optical flow and temporal consistent edge maps.
To achieve this goal, a flow-edge consistency loss is defined as:
\begin{equation}
\label{eq:flow_edge}
\mathcal{L}_{fec}=\sum_{k}\frac{1}{\left\|M_{t} \right\|_1}\left\|(E_{t}-\phi(O_{t\Rightarrow t-k},E_{t-k}))\odot M_{t}\right\|_1,
\end{equation}
where $\phi(O_{t\Rightarrow t-k},E_{t-k})$ is the flow warping operation which uses optical flow $O_{t\Rightarrow t-k}$ to warp $E_{t-k}$.
$k$ denotes the index of neighboring frames ($k\in \left\{-7,-3,+3,+7 \right\}$). 
Specifically, $\mathcal{L}_{fec}$ warps the edge maps from neighboring frames to the target frame to penalize the reconstruction loss.
In terms of ENet, $\mathcal{L}_{fec}$ encourages the predicted edge maps to be temporal smoothing, which should conform to the motion tendency in the flow $\boldsymbol{O}$. 
In terms of FNet, $\mathcal{L}_{fec}$ constrains the model to focus on the edges in $\boldsymbol{E}$ by considering extra edge motion in Eq.~\ref{eq:flow_edge}. 
Consequently, besides of the loss terms $\mathcal{L}_{edge}$ and $\mathcal{L}_{flow}$ for ENet and FNet respectively, $\mathcal{L}_{fec}$ further enhances them to obtain temporally consistent edges and edge-clear flows.
Thus, the total loss function for joint training of ENet and FNet is:
\begin{equation}
\label{eq:flow}
\mathcal{L}_{joint}=\mathcal{L}_{edge}+\mathcal{L}_{flow}+ \mathcal{L}_{fec}.
\end{equation}












\subsection{Spatio-Temporal Inpainting (STI) Network}

With both the inpainted edges $\boldsymbol{E}$ and flows $\boldsymbol{O}$, a spatio-temporal inpainting (STI) network is designed to obtain the final target frame $Y_t$.
%under the guidance of the  and motion tendency, which is helpful to the completion process of the target frame...
Notably, the inpainted edges and flows provide structural details and temporal correlation, respectively.
To generate plausible video content, STI adopts a coarse-to-fine architecture, consisting of a coarse network and a refinement network, as shown in Fig.~\ref{fig:stiNet}.

%
Input of STI is the concatenation of $\boldsymbol{X}$, $\boldsymbol{E}$, and $\boldsymbol{M}$.
First, the coarse network is implemented with 3D convolutions to capture the temporal information, which targets to generate an initial rough completed frame. The 3D coarse network is able to incorporate information from neighboring frames by convolution of the time dimension.
%\cxj{Need refinement. why 3D convs works for generating complete frame?}
Second, the refinement network is implemented with 2D convolution layers to enhance structural details and refine the initial completed frame.

\begin{figure}[t]
	\centering
	\includegraphics[width=0.65\columnwidth]{SEM} % Reduce the figure size so that it is slightly narrower than the column. Don't use precise values for figure width.This setup will avoid overfull boxes. 
	\caption{The architecture of SEM.}
	\label{SEM}
\end{figure}

To fully exploit the structural information in $\boldsymbol{E}$, a structure enhancement mechanism (SEM) is designed for STI.
As shown in Fig.~\ref{SEM}, two separate convolution blocks are first applied to $\boldsymbol{E}$.
Then, the input feature map and extracted structural information are interacted via matrix multiplication. 
Next, a SoftMax operation is used to obtain a normalized attention map.
Besides the short-range edge information in $\boldsymbol{E}$, the normalized attention map contains extra long-range correlation between structure and high-level features of video content.
Finally, the normalized attention map is applied to the original video features, and the structure information is thus encoded in the intermediate features of STI.
After introducing structure information, the inpainted video content by STI becomes more structure-preserved and semantically realistic.

To smooth temporal flickers and aggregate complementary neighboring clues, the motion tendency information in $\boldsymbol{O}$ is utilized by warping the neighboring frames into the target frame and employing a frame reconstruction constraint.
Specifically, a flow warping loss is proposed:
\begin{equation}
\label{eq:inp_flow}
\mathcal{L}^I_{flo}=\sum_{\widehat{t}\in\mathcal{T}}\left\| Y_t-\phi(O_{t\Rightarrow \widehat{t}},Y_{\widehat{t}}) \right\|_1,
\end{equation}
where $\mathcal{T}=\{t-7,t-3\}$. $\phi(O_{t\Rightarrow \widehat{t}},Y_{\widehat{t}})$ warps $Y_{\widehat{t}}$ to $Y_{t}$ using flow $O_{t\Rightarrow \widehat{t}}$.
Therefore, $\mathcal{L}^I_{flo}$ expects that all the neighboring frames can be well warped to the target frame with small reconstruction loss, rendering the inpainted frames to be temporal consistent.

%\(O_{t\Rightarrow t-3}\) is used to warp the inpainted $Y_{t-3}$ to aid the current $Y_{t}$, which provides a high temporal coherence.


%, which will generate a temporal smooth fine-detailed $Y_{t}$.
Finally, the loss function for STI is:
\begin{equation}
\label{eq:inpain_all}
\mathcal{L}_{inp}=\mathcal{L}^{I}_{rec}+\lambda_2 * \mathcal{L}^I_{adv}+ \mathcal{L}^I_{flo},
\end{equation}
where $\mathcal{L}^{I}_{rec}$, $\mathcal{L}^I_{adv}$, and $\mathcal{L}^I_{flo}$ are respectively reconstruction loss, adversarial loss and flow warping loss.


Inspired by \cite{nazeri2019edgeconnect}, $\mathcal{L}^{I}_{rec}$ consists of three terms by:
\begin{equation}
\mathcal{L}^{I}_{rec}=\mathcal{L}^{I}_{l1}+\lambda_3 *\mathcal{L}^{I}_{per}+\lambda_4 *\mathcal{L}^{I}_{sty},
\end{equation}
%\begin{equation}
%\begin{aligned}
%\mathcal{L}^{I}_{rec}=&\frac{1}{\left\|\boldsymbol{M} \right\|_1}\left\|(\boldsymbol{Y}^i-\widetilde{\boldsymbol{Y}})\odot \boldsymbol{M}\right\|_1 +\frac{1}{\left\|M_t \right\|_1}\left\|(Y_t-\widetilde{Y}_t)\odot M_t\right\|_1\\
%&+\mathbb{E}[\sum_{j}\frac{1}{N_j}\left\|\boldsymbol{\psi}_j(\widetilde{Y}_t)-\boldsymbol{\psi}_j(Y_t)\right\|_1]\\
%&+\mathbb{E}_j[\left\|G^{\boldsymbol{\psi}_j}(\widetilde{Y}_t)-G^{\boldsymbol{\psi}_j}(Y_t)\right\|_1],
%\end{aligned}
%\end{equation}
where the first term is pixel $l_1$ loss to evaluate generation quality. Different from \cite{nazeri2019edgeconnect}, we penalize both the coarse and fine predictions, which is defined by:
\begin{equation}
\begin{aligned}
\mathcal{L}^{I}_{rec}&=\frac{1}{\left\|\boldsymbol{M} \right\|_1}\left\|(\boldsymbol{Y}^i-\widetilde{\boldsymbol{Y}})\odot \boldsymbol{M}\right\|_1\\ &+\lambda_5*\frac{1}{\left\|M_t \right\|_1}\left\|(Y_t-\widetilde{Y}_t)\odot M_t\right\|_1.
\end{aligned}
\end{equation}
%

The second term is the perceptual loss \cite{gatys2015neural}, which compares the semantic contents between the generated frame and ground truth. The third term is the style loss, designed to prevent "checkerboard" artifacts \cite{Sajjadi_2017_ICCV} caused by transpose convolution.




%$\boldsymbol{\psi}_j$ denotes the activation maps of $j_{th}$ layer in a network. $N_j$ is the number of elements in layer $\boldsymbol{\psi}_j$. We use layers $relu_{1\_1}$, $relu_{2\_1}$, $relu_{3\_1}$, $relu_{4\_1}$ and $relu_{5\_1}$ of the VGG-19 network pre-trained on the ImageNet dataset for this loss.
%The third term is the style loss, designed to prevent "checkerboard" artifacts \cite{Sajjadi_2017_ICCV} caused by transpose convolution. $G^{\boldsymbol{\psi}_j}$ is the Gram matrix calculated by auto-correlation of $\boldsymbol{\psi}_j$. The layers are the same as that of perceptual loss.


$\mathcal{L}^I_{adv}$ is defined as:
\begin{equation}
\label{eq:inp_adver}
\mathcal{L}^I_{adv}=\min\limits_{G^I} \max \limits_{D^I}\mathbb{E}[logD^I(\widetilde{Y}_t)]+\mathbb{E}log[1-D^I(Y_{t})],
\end{equation}
where $G^I$ is the STI and $D^I$ possesses the same architecture as that of $D^E$, except the input dimension.